{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1AAZyn5TIYG3zWdTNtLGBLo2O8QokPRh8",
      "authorship_tag": "ABX9TyO442Woq6X4cBVdrshbTWDv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zilliax-Barry/FYP_Deep-Learning-and-WiFi-based-Gesture-Recognition/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zzpKPJeVJeJ7",
        "outputId": "7a1a63d2-0213-485a-9fa3-b1183c30933a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jjr-pHTRWpDS",
        "outputId": "42b7f8d1-af18-4f96-fac5-f75489158d8a"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.4.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.9.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.4)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.1.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import pandas as pd\n",
        "import multiprocessing as mp\n",
        "import datetime\n",
        "#from helpers.duallogger import loggersetup\n",
        "#from helpers.filehelper import is_not_empty_file_exists, write_to_file, load_from_file\n",
        "import logging\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import re\n",
        "import heapq\n",
        "import collections\n",
        "import operator\n",
        "import numpy as np\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm"
      ],
      "metadata": {
        "id": "Bcyf7TleXkCv"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Process"
      ],
      "metadata": {
        "id": "pVDkK6vBZhZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.io.json import json_normalize\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "import json\n",
        "\n",
        "count = 0\n",
        "# 'reviewerID', 'asin', 'reviewerName', 'helpful', 'reviewText',  'overall', 'summary', 'unixReviewTime', 'reviewTime'\n",
        "reviewerID = []\n",
        "asin = []\n",
        "reviewerName = []\n",
        "helpful = []\n",
        "reviewText = []\n",
        "overall = []\n",
        "summary = []\n",
        "unixReviewTime = []\n",
        "reviewTime = []\n",
        "\n",
        "with open('./Grocery_and_Gourmet_Food_5.json') as f:\n",
        "  for line in f:\n",
        "    # line type: 'string'\n",
        "    # {\"reviewerid\": \"a3aeguhvtc5yf4\", \"asin\": \"b000edm70y\"...}    \n",
        "    dicts= json.loads(line.strip())\n",
        "    # print(dicts)\n",
        "    count = count + 1\n",
        "    if 'reviewerID' in dicts:\n",
        "      reviewerID.append(dicts['reviewerID'])\n",
        "    else:\n",
        "      reviewerName.append('noID')  \n",
        "    if 'reviewerName' in dicts:\n",
        "      reviewerName.append(dicts['reviewerName'])\n",
        "    else:\n",
        "      reviewerName.append('noName')  \n",
        "    if 'reviewText' in dicts:\n",
        "      reviewText.append(dicts['reviewText'])\n",
        "    else:\n",
        "      reviewText.append('noName')  \n",
        "\n",
        "print(count)  \n",
        "\n"
      ],
      "metadata": {
        "id": "jsrzpYjWoXvc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1314b4e-caf8-4189-eb9e-b7959a4fb232"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "151254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the nlp object\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "print(nlp.pipe_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDW-2w2NCT21",
        "outputId": "82249b1f-19bd-4eef-cf75-a8e6f753282a"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pre-process the reviewerText\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stopwords_nltk = stopwords.words('english')\n",
        "stopwords_spacy = list(STOP_WORDS)\n",
        "stopwords_spacy.append('\\n')\n",
        "stopwords = stopwords_nltk + list(set(stopwords_spacy) - set(stopwords_nltk))\n",
        "\n",
        "print(\"sw nltk: \", len(stopwords_nltk))\n",
        "print(\"sw spacy: \", len(stopwords_spacy))\n",
        "print(\"combined: \", len(stopwords))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWMBqZ4IFjoY",
        "outputId": "4261813f-506c-4610-98c7-2a1f1d513911"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sw nltk:  179\n",
            "sw spacy:  327\n",
            "combined:  383\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pre-process the reviewerText\n",
        "global prenlp\n",
        "prenlp = spacy.load(\"en_core_web_sm\", disable=['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner'])\n",
        "prenlp.add_pipe('sentencizer')\n",
        "punctuations = '!\"#$%&\\'()*+,-/:;<=>?@[\\\\]^_`{|}~Â©.'\n",
        "extractText = []\n",
        "\n",
        "for i in range(len(reviewText)):\n",
        "  text = reviewText[i].lower()\n",
        "  review = prenlp(text)\n",
        "  # POS tagging & tokenize\n",
        "  sentence = nlp(review.text, disable=['parser', 'ner'])\n",
        "  tokens = [tok.lemma_.lower().strip() for tok in sentence if tok.lemma_ != '-PRON-']\n",
        "  tokens = [tok for tok in tokens if tok not in stopwords and tok not in punctuations]\n",
        "  tokens = ' '.join(tokens)\n",
        "  extractText.append(tokens)\n",
        "\n"
      ],
      "metadata": {
        "id": "L1UBF5MNMU9x"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extractSentence = []\n",
        "for i in range(len(reviewText)):\n",
        "  doc = prenlp(reviewText[i]) \n",
        "  extractSentence.append([])\n",
        "  for idx, sentence in enumerate(doc.sents):\n",
        "      extractSentence[i].append(sentence.text)      "
      ],
      "metadata": {
        "id": "COeb4NISiMWB"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(reviewText[300])\n",
        "print(extractText[300])\n",
        "print(extractSentence[300])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIGz-vrzmtJv",
        "outputId": "fd31b11a-8c14-4c89-ff4c-c9672d2ec983"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this ghost combo sauce hits different areas of the mouth than the red/orange/yellow/chocolate Habanerohighly recommended for a spice lovers aresenalfor best kick results combine with another multi-Habanero saucemade in costa rica\n",
            "ghost combo sauce hit different area mouth red orange yellow chocolate habanerohighly recommend spice lover aresenalfor good kick result combine multi habanero saucemade costa rica\n",
            "['this ghost combo sauce hits different areas of the mouth than the red/orange/yellow/chocolate Habanerohighly recommended for a spice lovers aresenalfor best kick results combine with another multi-Habanero saucemade in costa rica']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Review Summarizer"
      ],
      "metadata": {
        "id": "d8LmRD6vsWoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Collection Frequency\n",
        "contractions = { \n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"needn't\": \"need not\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there had\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who's\": \"who is\",\n",
        "\"won't\": \"will not\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you're\": \"you are\"\n",
        "}\n",
        "\n",
        "word_token = {}\n",
        "cleanText = []\n",
        "\n",
        "for i in range(len(extractText)):\n",
        "  #print(reviewText[i])\n",
        "  eT_join = extractText[i]\n",
        "  # eT_join the text in the ith reviewText  \n",
        "  # words divide maynot only by '.' \n",
        "  eT_join = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', eT_join)\n",
        "  cleanText.append(eT_join)\n",
        "  #print(eT_join)\n",
        "  token = re.findall(r\"[\\w']+\", eT_join)\n",
        "  #print(token)\n",
        "  #\n",
        "  text = prenlp(cleanText[i])\n",
        "  for sent in text.sents:\n",
        "    for tok in sent:\n",
        "      #print('============')\n",
        "      #print(tk)\n",
        "      if str(tok.lemma_) not in contractions:\n",
        "        #print(tk)\n",
        "        if str(tok.lemma_) not in word_token:\n",
        "          word_token[str(tok.lemma_)] = 1\n",
        "        else:\n",
        "          word_token[str(tok.lemma_)] += 1\n",
        "\n",
        "print(word_token)\n",
        "\n"
      ],
      "metadata": {
        "id": "H-oPHxSEhshM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efc77ae7-323c-42bf-8f64-031735cfbca8"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'': 6503535}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(reviewText[400])\n",
        "print(extractText[400])\n",
        "print(cleanText[400])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wckxtuF2lVIQ",
        "outputId": "fea42e7f-35c8-44c2-a615-2381556a6817"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: if you buy this, you will eat ALL the fruit you buy ALL at once. You will use fruit as a flavor vehicle for Tajin. This is the best, best, best. AMAZING on oranges, grapes, apples and, of course, watermelon.\n",
            "warning buy eat fruit buy use fruit flavor vehicle tajin good good good amazing orange grape apple course watermelon\n",
            "warning buy eat fruit buy use fruit flavor vehicle tajin good good good amazing orange grape apple course watermelon\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# word_token\n",
        "print(\"Numbers of tokens:\", len(word_token))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otdSwYyZmJXq",
        "outputId": "fe010297-7658-4fb9-b7de-58fb4b748aab"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numbers of tokens: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cos_sim(a, b):\n",
        "  return dot(a, b)/(norm(a)*norm(b))"
      ],
      "metadata": {
        "id": "m7uqkFyWkSHJ"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose one reviewText to summarize"
      ],
      "metadata": {
        "id": "dzowKX-tZnC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#choose review index\n",
        "from heapq import nlargest\n",
        "\n",
        "index = 200\n",
        "\n",
        "print('Review Text: '+ reviewText[index])\n",
        "nouns = {}\n",
        "adj = {}\n",
        "sen_token = {}\n",
        "\n",
        "# Make the spaCy doc obj for the sentence\n",
        "renlp = spacy.load(\"en_core_web_sm\")\n",
        "text = renlp(cleanText[index])\n",
        "\n",
        "for sent in text.sents:\n",
        "  for tok in sent:\n",
        "    print(tok, tok.pos_, tok.lemma_)\n",
        "    if str(tok.lemma_) not in sen_token:\n",
        "      sen_token[str(tok.lemma_)] = 1\n",
        "    else:\n",
        "      sen_token[str(tok.lemma_)] += 1\n",
        "\n",
        "    if str(tok.pos_) == 'NOUN':\n",
        "      if str(tok.lemma_) not in nouns:\n",
        "        nouns[str(tok.lemma_)] = 1\n",
        "      else:\n",
        "        nouns[str(tok.lemma_)] += 1\n",
        "        \n",
        "    elif str(tok.pos_) == 'ADJ':\n",
        "      if str(tok.lemma_) not in adj:\n",
        "        adj[str(tok.lemma_)] = 1\n",
        "      else:\n",
        "        adj[str(tok.lemma_)] += 1\n",
        "\n",
        "print(sen_token)\n",
        "print(nouns)\n",
        "print(adj)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "903L7L9UY6kN",
        "outputId": "25dab92c-5c1d-47de-ae15-a010235e2118"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review Text: fragrant, evocative, delicious and very lovely.  These are not words usually used to describe tea.  I am sure this tea is loaded with who knows what flavorings & spices etc but in a steaming cup, this is heaven.  with some milk,  heaven plus\n",
            "fragrant PROPN fragrant\n",
            "evocative ADJ evocative\n",
            "delicious ADJ delicious\n",
            "lovely ADJ lovely\n",
            "word NOUN word\n",
            "usually ADV usually\n",
            "use VERB use\n",
            "describe NOUN describe\n",
            "tea NOUN tea\n",
            "sure ADJ sure\n",
            "tea NOUN tea\n",
            "load NOUN load\n",
            "know VERB know\n",
            "flavoring VERB flavor\n",
            "spice NOUN spice\n",
            "etc X etc\n",
            "steaming NOUN steaming\n",
            "cup NOUN cup\n",
            "heaven PROPN heaven\n",
            "milk NOUN milk\n",
            "heaven PROPN heaven\n",
            "plus CCONJ plus\n",
            "{'fragrant': 1, 'evocative': 1, 'delicious': 1, 'lovely': 1, 'word': 1, 'usually': 1, 'use': 1, 'describe': 1, 'tea': 2, 'sure': 1, 'load': 1, 'know': 1, 'flavor': 1, 'spice': 1, 'etc': 1, 'steaming': 1, 'cup': 1, 'heaven': 2, 'milk': 1, 'plus': 1}\n",
            "{'word': 1, 'describe': 1, 'tea': 2, 'load': 1, 'spice': 1, 'steaming': 1, 'cup': 1, 'milk': 1}\n",
            "{'evocative': 1, 'delicious': 1, 'lovely': 1, 'sure': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# most frqe noun sentence\n",
        "max_noun = nlargest(1, nouns, key = nouns.get)\n",
        "        \n",
        "print('max_noun', max_noun)\n",
        "print(cleanText[index])\n",
        "\n",
        "# \\s{1,} to ignore the other .'s\n",
        "sentences = re.split(\"[.!?]\\s{1,}\", reviewText[index])\n",
        "print(sentences)\n",
        "# a quick clean for each sentences\n",
        "extractSentence = []\n",
        "cleanSentence = []\n",
        "for s in sentences:\n",
        "  for i in range(len(s)):\n",
        "    t = s[i].lower()\n",
        "    review = prenlp(text)\n",
        "    # POS tagging & tokenize\n",
        "    d = nlp(review.text, disable=['parser', 'ner'])\n",
        "    k = [tok.lemma_.lower().strip() for tok in d if tok.lemma_ != '-PRON-']\n",
        "    k = [tok for tok in k if tok not in stopwords and tok not in punctuations]\n",
        "    k = ' '.join(k)\n",
        "    extractSentence.append(k)\n",
        "\n",
        "for i in range(len(extractSentence)):\n",
        "  #print(extractSentence[i])\n",
        "  eS_join = extractSentence[i]\n",
        "  # eS_join the sentence in the ith extractSentence  \n",
        "  # words divide maynot only by '.' \n",
        "  eS_join = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', eS_join)\n",
        "  cleanText.append(eS_join)\n",
        "\n",
        "print('=====================')\n",
        "print(extractSentence)\n",
        "print(cleanText)\n",
        "# tf-idf for a sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_f2gOCmNfNx",
        "outputId": "ce222c73-f3e7-472d-d655-9624cec1e272"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_noun ['tea']\n",
            "fragrant evocative delicious lovely word usually use describe tea sure tea load know flavoring spice etc steaming cup heaven milk heaven plus\n",
            "['fragrant, evocative, delicious and very lovely', 'These are not words usually used to describe tea', 'I am sure this tea is loaded with who knows what flavorings & spices etc but in a steaming cup, this is heaven', 'with some milk,  heaven plus']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}